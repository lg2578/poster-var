越大的模型迁移能力越强？
resnet50比convnextv2更容易训练？训练时波动小？ 可能是模型较小，不容易过拟合
学习率下降并不是约慢越好
cache_image 速度变慢？，isic2024 valid
我们可以用数字化的方式来表示这个世界，如色彩我们可以用rgb三原色来数字化实现。这就回到图灵的可计算数问题
预训练模型适合用较小的学习率，因为已经训练的比较充分，大的学习率可能会让模型变差。

***向量对齐学习：每个专用模型都可以和大语言模型做向量对齐，大语言模型可以通过比较最接近的向量来选择专用模型，在某个场景下，用这个专用模型来进行决策，从而大大降低推理阶段的模型大小。并且可以用很多小模型，来更快，更准确的完成任务。
***对错误标定进行修正，对比学习可能会效果更好？

embedding可以训练，但好的embedding如何获得？
20250209
---1.去掉warmup，如果要warmup，可以先用低学习率训练一个初始模型，再load继续训练
---2.学习率的调整都在batch中，既可以设置成按batch调整，也可以按epoch调整
---3.换成wandb，最佳模型保存在本地，更好的保存参数的变化
4.验证下embedding onehot等价于label
---5.损失函数可以配置，计算损失写成一个函数
6.表情数据集测试下
20250218
7.vae 在cub200上能用，但未超过原模型，可以延长训练epoch和采用cosine退火，因为拟合度比原模型低，这是一个提升空间。在scut上，貌似落后原模型，是不是学习率要小点？
=》cosine退火效果：
=》延长epochs效果：


创新点：
1.改进的cosine退火
2.vae做正态分布采样，vae2在train的时候做采样，在eval的时候直接返回mu，vae3没有dropout
可能dropout有效，而vae2并没有效果？vae4，dropout在最后进行，拟合会变差很多，泛化好像影响不大
vae3效果最好
3.对比学习



